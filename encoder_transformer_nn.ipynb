{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "NXdrw16L3CDO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "max_sequence_length = 200"
      ],
      "metadata": {
        "id": "ntZIz5Ui3CA2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder"
      ],
      "metadata": {
        "id": "G6EfS-iv0H98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode(nn.Module):\n",
        "  def __init__(self,d_model,num_heads,drop_prob,ffn_hidden,num_layers):\n",
        "    super().__init__()\n",
        "    self.encode_layer = nn.Sequential(*[Encode_layer(d_model,num_heads,drop_prob,ffn_hidden) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.encode_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xfA5PO7n2wOT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MultiheadAttention"
      ],
      "metadata": {
        "id": "mAIKzRyf0J2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super(MultiheadAttention,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.qkv_layer = nn.Linear(d_model,3*d_model)\n",
        "    self.linear_layer = nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    qkv = self.qkv_layer(x)# 30 X 200 X 1536\n",
        "    qkv = qkv.reshape(batch_size,self.num_heads,max_sequence_length,3*self.head_dim)# 30 X 8 X 200 X 192\n",
        "    q,k,v = qkv.chunk(3,dim=-1)\n",
        "    d_k = q.shape[-1]\n",
        "    scaled = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)# 30 X 8 X 200 X 192\n",
        "    if mask is not None:\n",
        "      scaled += mask\n",
        "    self_attention = torch.softmax(scaled,dim=-1)# 30 X 8 X 200 X 192\n",
        "    out = torch.matmul(self_attention,v)# 30 X 8 X 200 X 192\n",
        "    out = out.reshape(batch_size,max_sequence_length,self.num_heads*self.head_dim)# 30 X 200 X 512\n",
        "    out = self.linear_layer(out)# 30 X 200 X 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "Gb3ob5kIEvh2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LayerNormalization"
      ],
      "metadata": {
        "id": "Pwq1OAKL0L8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,parameter_size,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameter_size = parameter_size#[512]\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(parameter_size))#[512]\n",
        "    self.beta = nn.Parameter(torch.zeros(parameter_size))#[512]\n",
        "\n",
        "  def forward(self,x):\n",
        "    dims = [(-i+1) for i in range(len(self.parameter_size))]#[-1]\n",
        "    mean = x.mean(dim=dims, keepdim=True)# 30 x 200 x 1\n",
        "    var=((x-mean)**2).mean(dim=dims, keepdim=True)# 30 x 200 x 1\n",
        "    std_ = (var+self.eps).sqrt()# 30 x 200 x 1\n",
        "    y = (x - mean) / std_# 30 x 200 x 512\n",
        "    out = self.gamma * y + self.beta# 30 x 200 x 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "xNoferobEtEy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PositionwiseFeedForward"
      ],
      "metadata": {
        "id": "xc3W3q9_0OuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,drop_prob):\n",
        "    super().__init__()\n",
        "    self.linear_layer_1 = nn.Linear(d_model,ffn_hidden)#512 x 2048\n",
        "    self.linear_layer_2 = nn.Linear(ffn_hidden,d_model)#2048 x 512\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=drop_prob)\n",
        "  def forward(self,x):\n",
        "    x = self.linear_layer_1(x)# 30 x 200 x 2048\n",
        "    x = self.relu(x)# 30 x 200 x 2048\n",
        "    x = self.dropout(x)# 30 x 200 x 2048\n",
        "    x = self.linear_layer_2(x)# 30 x 200 x 512\n",
        "    return x"
      ],
      "metadata": {
        "id": "yu8aqpIGEqs9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encode_layer"
      ],
      "metadata": {
        "id": "kxd7CpWe0Qku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode_layer(nn.Module):\n",
        "  def __init__(self,d_model,num_heads,drop_prob,ffn_hidden):\n",
        "    super(Encode_layer,self).__init__()\n",
        "    self.attention = MultiheadAttention(d_model,num_heads)\n",
        "    self.norm1 = LayerNormalization([d_model])\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)\n",
        "    self.norm2 = LayerNormalization([d_model])\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    print(\"Starting epoch...\")\n",
        "    residual_x = x # 30 x 200 x 512\n",
        "    x = self.attention(x) # 30 X 200 X 512\n",
        "    print(\"Attention done.\")\n",
        "    x = self.dropout1(x)# 30 X 200 X 512\n",
        "    x = self.norm1(x+residual_x)# 30 X 200 X 512\n",
        "    print(\"normalization completed\")\n",
        "    residual_x = x# 30 X 200 X 512\n",
        "    x = self.ffn(x)# 30 X 200 X 512\n",
        "    print(\"feed forwarded\")\n",
        "    x = self.dropout2(x)# 30 X 200 X 512\n",
        "    x = self.norm2(x+residual_x)# 30 X 200 X 512\n",
        "    print(\"normalization completed\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "81sZ_gGo5rkU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ],
      "metadata": {
        "id": "pwb6KR4X5s0p"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random input"
      ],
      "metadata": {
        "id": "cePGuU-35syE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((batch_size,max_sequence_length,d_model))"
      ],
      "metadata": {
        "id": "0OmPC8h9FAXb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encode(d_model,num_heads,drop_prob,ffn_hidden,num_layers)"
      ],
      "metadata": {
        "id": "D1oyKYJ4FAVF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = encoder(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA8GjJN_Faw7",
        "outputId": "5f5b6337-b036-4a7e-d525-d4902541908f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0AeK5O_HsSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}