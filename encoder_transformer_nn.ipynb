{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "NXdrw16L3CDO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "max_sequence_length = 200"
      ],
      "metadata": {
        "id": "ntZIz5Ui3CA2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode(nn.Module):\n",
        "  def __init__(self,d_model,num_heads,drop_prob,ffn_hidden,num_layers):\n",
        "    super().__init__()\n",
        "    self.encode_layer = nn.Sequential(*[Encode_layer(d_model,num_heads,drop_prob,ffn_hidden) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.encode_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xfA5PO7n2wOT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super(MultiheadAttention,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.qkv_layer = nn.Linear(d_model,3*d_model)\n",
        "    self.linear_layer = nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    qkv = self.qkv_layer(x)# 30 X 200 X 1536\n",
        "    qkv = qkv.reshape(batch_size,self.num_heads,max_sequence_length,3*self.head_dim)# 30 X 8 X 200 X 192\n",
        "    q,k,v = qkv.chunk(3,dim=-1)\n",
        "    d_k = q.shape[-1]\n",
        "    scaled = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)# 30 X 8 X 200 X 192\n",
        "    if mask is not None:\n",
        "      scaled += mask\n",
        "    self_attention = torch.softmax(scaled,dim=-1)# 30 X 8 X 200 X 192\n",
        "    out = torch.matmul(self_attention,v)# 30 X 8 X 200 X 192\n",
        "    out = out.reshape(batch_size,max_sequence_length,self.num_heads*self.head_dim)# 30 X 200 X 512\n",
        "    out = self.linear_layer(out)# 30 X 200 X 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "Gb3ob5kIEvh2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,parameter_size,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameter_size = parameter_size#[512]\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(parameter_size))#[512]\n",
        "    self.beta = nn.Parameter(torch.zeros(parameter_size))#[512]\n",
        "\n",
        "  def forward(self,x):\n",
        "    dims = [(-i+1) for i in range(len(self.parameter_size))]#[-1]\n",
        "    mean = x.mean(dim=dims, keepdim=True)# 30 x 200 x 1\n",
        "    var=((x-mean)**2).mean(dim=dims, keepdim=True)# 30 x 200 x 1\n",
        "    std_ = (var+self.eps).sqrt()# 30 x 200 x 1\n",
        "    y = (x - mean) / std_# 30 x 200 x 512\n",
        "    out = self.gamma * y + self.beta# 30 x 200 x 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "xNoferobEtEy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,drop_prob):\n",
        "    super().__init__()\n",
        "    self.linear_layer_1 = nn.Linear(d_model,ffn_hidden)#512 x 2048\n",
        "    self.linear_layer_2 = nn.Linear(ffn_hidden,d_model)#2048 x 512\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=drop_prob)\n",
        "  def forward(self,x):\n",
        "    x = self.linear_layer_1(x)# 30 x 200 x 2048\n",
        "    x = self.relu(x)# 30 x 200 x 2048\n",
        "    x = self.dropout(x)# 30 x 200 x 2048\n",
        "    x = self.linear_layer_2(x)# 30 x 200 x 512\n",
        "    return x"
      ],
      "metadata": {
        "id": "yu8aqpIGEqs9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode_layer(nn.Module):\n",
        "  def __init__(self,d_model,num_heads,drop_prob,ffn_hidden):\n",
        "    super(Encode_layer,self).__init__()\n",
        "    self.attention = MultiheadAttention(d_model,num_heads)\n",
        "    self.norm1 = LayerNormalization([d_model])\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)\n",
        "    self.norm2 = LayerNormalization([d_model])\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    print(\"Starting epoch...\")\n",
        "    residual_x = x # 30 x 200 x 512\n",
        "    x = self.attention(x) # 30 X 200 X 512\n",
        "    print(\"Attention done.\")\n",
        "    x = self.dropout1(x)# 30 X 200 X 512\n",
        "    x = self.norm1(x+residual_x)# 30 X 200 X 512\n",
        "    print(\"normalization completed\")\n",
        "    residual_x = x# 30 X 200 X 512\n",
        "    x = self.ffn(x)# 30 X 200 X 512\n",
        "    print(\"feed forwarded\")\n",
        "    x = self.dropout2(x)# 30 X 200 X 512\n",
        "    x = self.norm2(x+residual_x)# 30 X 200 X 512\n",
        "    print(\"normalization completed\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "81sZ_gGo5rkU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ],
      "metadata": {
        "id": "pwb6KR4X5s0p"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random input"
      ],
      "metadata": {
        "id": "cePGuU-35syE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((batch_size,max_sequence_length,d_model))"
      ],
      "metadata": {
        "id": "0OmPC8h9FAXb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encode(d_model,num_heads,drop_prob,ffn_hidden,num_layers)"
      ],
      "metadata": {
        "id": "D1oyKYJ4FAVF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = encoder(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA8GjJN_Faw7",
        "outputId": "5f5b6337-b036-4a7e-d525-d4902541908f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n",
            "Starting epoch...\n",
            "Attention done.\n",
            "normalization completed\n",
            "feed forwarded\n",
            "normalization completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojh747A3GFxV",
        "outputId": "7e53cfec-a0c1-4917-c6a5-edcd4f8811f5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.1239e-01,  6.5751e-01, -9.9169e-01,  1.4961e+00,  5.0408e-01,\n",
              "         3.1691e-02,  1.9468e-01, -1.3889e+00, -3.1518e-01,  1.4601e+00,\n",
              "         2.3780e+00,  2.2442e-01, -1.4515e+00, -9.0728e-01,  1.1531e-01,\n",
              "         1.0405e+00, -1.3333e-01,  9.7122e-02, -5.2967e-01, -1.7226e+00,\n",
              "        -3.1487e+00, -1.1113e+00, -7.4371e-01, -8.7416e-01,  1.5411e+00,\n",
              "         5.0356e-01, -2.6564e-01,  1.7900e-01, -8.1131e-02,  6.3473e-01,\n",
              "         2.6696e-01,  5.7962e-01,  9.8714e-02,  9.1608e-01,  8.4464e-01,\n",
              "         9.4181e-01,  3.6722e-01, -7.6956e-01,  1.2320e+00,  1.0828e+00,\n",
              "         1.2177e+00,  2.0513e-01,  2.7616e-01, -1.0094e+00, -2.2355e-01,\n",
              "        -9.2582e-01,  6.7670e-01,  8.0102e-01,  2.4730e-01,  1.3716e-01,\n",
              "        -4.8882e-01,  1.5830e+00,  3.5139e-01, -3.9447e-01, -7.7217e-01,\n",
              "        -1.4764e+00,  7.7121e-01,  1.5478e+00,  2.0299e-01,  1.1259e+00,\n",
              "         8.5689e-01, -1.5605e+00, -8.5204e-01,  5.7054e-01, -2.9758e-01,\n",
              "         5.8265e-01, -8.0308e-01, -4.1799e-01, -2.4126e-01, -4.2667e-01,\n",
              "        -2.8168e-01, -1.0539e+00, -1.3567e+00,  2.4497e-02,  1.4067e+00,\n",
              "        -9.9548e-01, -1.7361e+00,  1.0758e+00,  1.1946e+00, -5.1795e-02,\n",
              "        -2.8489e-01,  1.7272e+00, -7.7066e-02,  7.5313e-01,  1.5921e+00,\n",
              "        -2.3667e+00,  7.2445e-01,  5.4758e-01, -2.3181e-03, -1.0079e+00,\n",
              "         5.0675e-03, -7.6117e-01, -1.0170e+00, -2.5136e-01, -1.0436e+00,\n",
              "         2.0407e+00,  1.9193e+00,  7.0834e-01, -7.4361e-01,  1.8761e+00,\n",
              "        -1.2421e+00, -1.1263e+00, -3.7874e-01,  1.5192e-01,  1.0431e+00,\n",
              "        -1.3380e-01,  8.2617e-01, -5.8186e-01, -4.0007e-01, -1.9307e+00,\n",
              "         9.2722e-01, -8.9935e-01,  1.3876e-01, -1.9381e+00, -3.9716e-01,\n",
              "         4.8259e-01,  3.9575e-01,  5.4238e-01,  6.8653e-01,  4.7613e-01,\n",
              "         3.7139e-01, -1.8860e-01, -4.2077e-01,  1.7271e+00,  2.4487e-01,\n",
              "        -5.7528e-01, -4.9060e-01,  8.2010e-01,  2.0247e+00,  1.0396e+00,\n",
              "         6.7691e-01,  1.2646e+00, -3.5568e-01, -2.1674e+00, -2.2371e-01,\n",
              "        -1.6284e-02, -8.3806e-01, -1.3652e+00, -8.7817e-01, -1.7445e+00,\n",
              "        -5.3094e-01, -1.9416e+00, -1.0045e+00,  1.8762e-02,  4.2948e-01,\n",
              "        -2.3529e-01, -1.3150e+00,  1.5384e+00, -1.2166e+00,  1.6987e-01,\n",
              "        -6.5111e-01, -1.1800e+00,  6.8870e-02, -1.2617e+00,  1.3543e+00,\n",
              "        -1.0718e+00, -1.0137e+00, -7.3036e-01,  1.7334e+00,  3.9179e-01,\n",
              "         1.0884e+00,  8.3759e-01,  6.8140e-01, -1.8277e+00, -2.3422e-01,\n",
              "        -3.9405e-01,  2.9236e-01,  3.2018e-01, -1.3825e+00,  1.2207e+00,\n",
              "        -8.6461e-01,  1.1962e+00, -7.4887e-01, -1.3075e+00,  1.4019e+00,\n",
              "         3.7069e-01,  1.5947e+00,  5.6325e-01,  2.3602e-01,  6.1233e-03,\n",
              "         4.1122e-01, -2.5677e-01,  4.2231e-01,  1.0009e-01, -4.2756e-01,\n",
              "         6.0360e-01,  2.2174e-01,  1.0745e+00, -9.1009e-01,  7.0964e-01,\n",
              "         4.1060e-01, -3.7443e-01,  8.8484e-01,  1.4592e+00, -4.8867e-01,\n",
              "        -5.5123e-01, -1.4961e+00, -7.5223e-01, -2.0478e-01, -5.9463e-01,\n",
              "        -1.2712e-03, -1.3888e+00, -1.6990e+00,  8.9608e-01,  1.9933e-01,\n",
              "         3.4691e-02,  5.1773e-01,  2.9172e+00,  2.0434e+00, -8.7536e-02,\n",
              "        -8.9361e-03,  2.2329e+00,  7.5101e-01, -1.7811e+00,  1.3683e+00,\n",
              "        -2.2707e+00,  1.4267e+00, -1.1962e+00, -1.3887e-02, -6.8215e-01,\n",
              "         3.3086e-01, -7.2241e-01, -3.1617e-01, -1.9319e-01,  5.2856e-02,\n",
              "         1.4568e+00,  1.7579e+00,  5.9346e-01,  3.8929e-01,  1.3521e+00,\n",
              "         3.9779e-01, -1.0117e+00,  3.7306e-01, -6.1616e-01, -1.8021e+00,\n",
              "        -2.2755e+00, -7.0916e-01, -5.1045e-01, -2.9556e-02, -1.9936e+00,\n",
              "        -2.8164e-01, -1.9129e+00, -1.6481e-01, -4.4315e-01,  4.6609e-01,\n",
              "        -8.0878e-01, -5.4970e-02, -1.3776e+00, -1.2271e-01, -7.1067e-01,\n",
              "         1.5529e-01, -3.8766e-01, -1.2240e+00, -2.0767e-02, -9.0429e-01,\n",
              "         4.3035e-02, -3.2207e-01, -8.3603e-01,  1.5827e+00,  7.0007e-01,\n",
              "        -3.3472e-01,  9.9698e-01,  6.2233e-01,  7.0412e-02,  8.7557e-01,\n",
              "         2.7674e-01,  1.2588e+00,  1.3368e+00, -8.2766e-01,  1.2665e+00,\n",
              "        -6.2270e-01,  7.9639e-01,  4.1005e-01, -1.5741e+00,  4.6084e-01,\n",
              "         2.3651e+00,  2.7118e-01,  7.9859e-01,  7.1739e-01,  5.6566e-01,\n",
              "         5.6446e-01,  6.6188e-01,  4.3525e-01, -1.6540e+00,  2.5460e-01,\n",
              "        -7.0250e-01,  1.3342e+00, -7.1869e-01, -1.5251e+00,  1.0971e+00,\n",
              "         2.2629e+00,  1.2777e+00, -1.3244e-01, -1.7328e+00,  5.3702e-01,\n",
              "        -2.3261e-01, -3.0725e-01,  1.2282e+00, -3.8711e-02, -1.0023e+00,\n",
              "        -2.7224e-01,  1.4045e+00, -4.4013e-01, -4.2294e-01,  1.3488e+00,\n",
              "        -3.7874e-01, -4.9024e-01,  9.0218e-01,  1.4466e+00,  3.3627e-02,\n",
              "         1.1391e+00,  7.7250e-01, -1.4483e-01,  6.6156e-01, -8.2805e-01,\n",
              "         2.6220e-01,  2.1347e+00,  1.2272e+00,  4.0623e-01, -1.5760e+00,\n",
              "         2.9602e-01,  2.1760e-01, -7.8872e-02, -5.4787e-01, -6.3576e-01,\n",
              "        -6.2632e-01, -1.4533e+00,  1.4316e-01, -1.7500e+00,  1.1844e+00,\n",
              "         4.7412e-01,  4.8383e-01, -3.1699e+00,  2.1307e+00,  3.6057e-01,\n",
              "         1.6653e+00,  1.7000e+00,  5.5169e-01,  4.5956e-01,  1.5512e+00,\n",
              "         2.2403e-01,  9.8099e-02, -8.9243e-01,  2.3004e+00,  1.0643e-02,\n",
              "         1.4111e+00,  2.1411e+00,  7.2820e-01, -2.0135e+00, -4.0510e-01,\n",
              "         8.9179e-01,  1.3739e+00,  5.4667e-02, -2.1891e+00, -1.4843e+00,\n",
              "         1.2526e+00,  7.0557e-01,  2.3687e+00,  4.4269e-01,  1.2710e+00,\n",
              "        -3.7689e-01,  1.3111e+00, -6.8482e-03,  1.8472e-01, -1.0091e+00,\n",
              "         1.0596e+00,  2.6595e+00, -2.5594e+00,  9.3563e-01,  6.8130e-01,\n",
              "        -7.4808e-01, -4.4104e-01, -2.0918e+00,  1.1475e-01, -5.9062e-01,\n",
              "         8.5567e-01, -5.5703e-01, -1.3735e+00,  9.4457e-01, -1.4332e+00,\n",
              "         1.1496e+00,  1.4933e+00,  3.0002e-01, -1.5000e+00, -6.5304e-02,\n",
              "        -1.0311e+00, -1.3738e+00, -6.0741e-01, -2.8880e+00, -1.7735e+00,\n",
              "         2.5222e-01, -1.0570e+00,  8.0587e-01, -3.4054e-01,  1.8007e-01,\n",
              "         8.0007e-01, -1.1748e+00, -1.5622e+00,  5.2968e-01,  1.2141e+00,\n",
              "         3.1084e-01, -2.4317e-01, -5.4855e-01, -1.6020e+00, -1.3908e-01,\n",
              "         1.3318e-02, -1.0815e+00, -4.1420e-01, -2.0091e+00,  3.0740e-01,\n",
              "         2.1876e-01,  1.0837e+00,  3.9106e-01,  6.5807e-01,  4.2407e-01,\n",
              "         1.0436e+00,  5.9320e-01,  8.5531e-01,  1.4177e-02, -1.7370e+00,\n",
              "        -7.9334e-01,  8.4885e-01, -1.5345e+00,  1.6476e-01,  1.1593e+00,\n",
              "        -6.4426e-01,  1.2394e+00,  1.6592e+00,  3.8864e-01,  1.0399e+00,\n",
              "        -2.0149e-01,  8.7942e-01,  2.1975e-01,  1.7261e+00, -2.5033e-01,\n",
              "        -1.1689e+00, -1.8904e+00, -5.8504e-01, -9.6957e-01, -1.8296e-01,\n",
              "         4.0935e-01,  6.2535e-01, -1.5559e+00,  8.5001e-02, -2.4119e+00,\n",
              "         2.2736e-01,  9.4118e-01, -1.5639e+00,  1.1908e+00,  8.3832e-01,\n",
              "        -9.4138e-02, -1.7630e-01,  8.3118e-01, -5.3861e-01, -3.1186e-01,\n",
              "         2.8973e-01, -1.0750e+00,  5.1815e-01, -3.2356e-01, -1.6170e-02,\n",
              "        -1.3624e+00, -1.5602e+00,  6.5783e-01,  2.7197e-01,  1.3655e+00,\n",
              "         6.7575e-01,  6.4819e-01, -2.1459e-01, -1.0222e+00,  2.7972e-01,\n",
              "        -2.7407e-01,  1.2041e+00,  1.4640e-01, -1.3574e+00, -9.8234e-01,\n",
              "         6.1544e-01, -9.6306e-01, -7.0153e-01, -5.5631e-01, -2.0260e-01,\n",
              "        -3.8010e-01,  3.7933e-01, -1.7582e-01,  2.9490e+00,  1.7447e+00,\n",
              "        -9.8198e-01,  2.1360e+00,  1.0484e+00, -1.1482e+00, -9.3776e-01,\n",
              "        -2.5857e-01, -9.9532e-02, -2.5248e-01,  6.4901e-01,  5.9143e-01,\n",
              "        -5.0101e-01, -2.4600e+00,  1.9409e+00,  1.0981e+00, -7.2502e-02,\n",
              "        -6.2565e-01, -1.2584e-01, -8.3000e-01,  4.1460e-01,  2.8032e-01,\n",
              "         3.9412e-01,  4.2206e-01, -5.0906e-01,  7.3651e-01, -1.8081e+00,\n",
              "        -1.7589e+00,  1.3455e+00], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0AeK5O_HsSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}